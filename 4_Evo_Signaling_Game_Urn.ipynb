{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De2HkHqkre4W"
   },
   "source": [
    "# シグナリングゲームにおける進化的強化学習（ポリアの壺）\n",
    "\n",
    "このノートブックでは，ポリアの壺を用いた送信者と受信者によるシグナリングゲームにおいて，遺伝的な進化の要素を導入した進化的強化学習の実験を行います．\n",
    "ここでは遺伝子として，それぞれの壺に最初に入っているボールの数を決める遺伝子を考えます．これは，生まれながらにして本能的に行う行動のバイアスを決める遺伝子に対応します．\n",
    "\n",
    "それぞれの個体が一生のうちに得られる学習機会が少なかったり，コミュニケーションの失敗が致命的である場合などには，学習に頼らない本能的なコミュニケーションシステムが進化するかもしれません．\n",
    "ただし，ある遺伝子がコミュニケーションを成功させることに有用かどうかは，集団内の他の個体が持つ遺伝子や学習状況にも依存します．このため，首の長さを決める遺伝子など，個体それぞれで独立して適応度に影響を与える遺伝子よりも，進化の過程は複雑なものになることが予想されます．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzIwhMmVg5u2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Evauj16rdnN"
   },
   "source": [
    "まず，シグナリングゲームを表現する強化学習環境（`SignalingGameEnv`）を定義します．\n",
    "これは前回のシグナリングゲーム環境とほとんど同じですが，今回は遺伝的進化を扱うことから， **報酬** と **適応度** を区別する必要があることに注意してください．\n",
    "\n",
    "- 報酬は，出生後の学習を方向付けるための，個体内部のメカニズムによって与えられるものです．これは脳が出すホルモンなどに対応します．\n",
    "- 適応度は，次の世代に遺伝子を残すことができるかどうかを決めるための，個体の外部の環境のメカニズムによって決まるものです．\n",
    "\n",
    "適応度と報酬は，関連するように進化します．例えば，食物を摂取することは適応度を向上させますが，多くの生物は食物を摂取すると「おいしい」と感じる報酬を得て，その後食物の摂取を積極的に行うように学習されます．\n",
    "しかし，必ずしも適応度と報酬は一致しません．例えば，時々味覚障害をもつ個体が生まれることがありますが，この場合は食物の摂取が適応度を向上させるにもかかわらず，報酬を得られないことがあります．また，多くの哺乳類は子どもを愛おしく思う報酬を得るように進化していますが，これは自分自身の適応度を向上させるわけではなく，むしろ子どもを守るために自分を犠牲にするような行動に報酬を感じたりすることもあります．このように，適応度と報酬は必ずしも一致しないということを考慮することは，進化的強化学習の計算モデルを設計する上で重要です．\n",
    "\n",
    "ここでは，適応度や報酬をどう設定するかについて，後述するように実験条件としていくつかのパターンを試します．このため，以下の環境の定義では，`rewards` を直接与えるのではなく，`success` という変数で成功したかどうかだけを保持するようにしています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIrsOemzhZiK"
   },
   "outputs": [],
   "source": [
    "class SignalingGameEnv:\n",
    "    def __init__(self, num_states, num_messages, state_dist):\n",
    "        self.num_states = num_states\n",
    "        self.num_messages = num_messages\n",
    "        self.state_dist = state_dist\n",
    "        self.agent_names = ['sender', 'receiver']\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = rng.choice(self.num_states, p=self.state_dist)\n",
    "        self.success = None\n",
    "        self.done = False\n",
    "        self.agent_selection = 'sender'\n",
    "\n",
    "    def observe(self, agent_name):\n",
    "        if agent_name == 'sender':\n",
    "            return self.state\n",
    "        elif agent_name == 'receiver':\n",
    "            return self.message\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.done == False, 'WARNING: The game is done. Call reset().'\n",
    "        agent_name = self.agent_selection\n",
    "        if agent_name == 'sender':\n",
    "            self.message = action\n",
    "            self.agent_selection = 'receiver'\n",
    "        elif agent_name == 'receiver':\n",
    "            self.action = action\n",
    "            self.success = self.state == action\n",
    "            self.done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に，遺伝子を表現するクラス `Gene` を定義します．\n",
    "ここでは，ポリアの壺に最初に入っているボールの数を表現するための遺伝子を想定しています．\n",
    "これを表現するために，与えられた `size` の大きさで，それぞれの要素が `min_val` と `max_val` の間の値を取るようなNumpy配列を保持するクラスとして定義します．また，交叉や突然変異のメソッドを定義しています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gene:\n",
    "    def __init__(self, size, min_val, max_val, init_values='random'):\n",
    "        # size は整数または整数のタプルです．\n",
    "        self.size = size\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "        self.init_values = init_values\n",
    "        if init_values == 'random':\n",
    "            self.values = rng.uniform(self.min_val, self.max_val, size)\n",
    "        else:\n",
    "            self.values = np.full(size, init_values)\n",
    "\n",
    "    def crossover(self, other_gene):\n",
    "        new_gene = Gene(self.size, self.min_val, self.max_val, init_values=self.init_values)\n",
    "        random_mask = rng.integers(0, 2, self.size)\n",
    "        new_gene.values = np.where(random_mask, self.values, other_gene.values)\n",
    "        return new_gene\n",
    "\n",
    "    def mutate(self, mutation_rate):\n",
    "        random_mask = rng.uniform(0, 1, self.size) < mutation_rate\n",
    "        self.values = np.where(random_mask, rng.uniform(self.min_val, self.max_val, self.size), self.values)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xu9QTPUryyNj"
   },
   "source": [
    "次に，ポリアの壺モデルに基づく強化学習エージェントのクラス（`UrnAgent`）を定義します．\n",
    "これは，前回のノートブックで定義したものとほとんど同じですが，`Gene` クラスのオブジェクトを受け取って初期化するように変更しています．\n",
    "ここでは `size = (num_obs, num_actions)` で，`values` の $(i, j)$ 要素が壺 $i$ の中にボール $j$ が最初に入っている個数であるような `Gene` クラスのオブジェクトが与えられることを想定しています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Eg0SJxZiWVr"
   },
   "outputs": [],
   "source": [
    "class UrnAgent:\n",
    "    def __init__(self, bias_gene):\n",
    "        self.bias_gene = bias_gene\n",
    "        self.num_obs = bias_gene.size[0]\n",
    "        self.num_choices = bias_gene.size[1]\n",
    "        self.urn_balls = [np.ones(self.num_choices, dtype=float) for _ in range(self.num_obs)]\n",
    "        self.urn_sum_balls = [self.num_choices for _ in range(self.num_obs)]\n",
    "        self.train_buf = []\n",
    "        # Apply innate bias\n",
    "        for obs in range(self.num_obs):\n",
    "            self.urn_balls[obs] += self.bias_gene.values[obs]\n",
    "            self.urn_sum_balls[obs] = self.urn_balls[obs].sum()\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        p = self.urn_balls[obs] / self.urn_sum_balls[obs]\n",
    "        return rng.choice(np.arange(self.num_choices), p=p)\n",
    "\n",
    "    def store_buffer(self, obs, choice):\n",
    "        self.train_buf.append([obs, choice, 0]) # 観測，選択，報酬の組\n",
    "\n",
    "    def update_reward(self, reward):\n",
    "        if len(self.train_buf) > 0:\n",
    "          self.train_buf[-1][2] += reward\n",
    "\n",
    "    def train(self):\n",
    "        for obs, choice, reward in self.train_buf:\n",
    "            self.urn_balls[obs][choice] += reward\n",
    "            self.urn_sum_balls[obs] += reward\n",
    "        self.train_buf = []\n",
    "\n",
    "def str_agent(agent):\n",
    "    str_urn = ''\n",
    "    for obs in range(agent.num_obs):\n",
    "        for choice in range(agent.num_choices):\n",
    "            str_urn += f'{obs}->{choice}:{agent.urn_balls[obs][choice]:.0f}({agent.bias_gene.values[obs][choice]:.0f}), '\n",
    "        str_urn += '\\n'\n",
    "    return str_urn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また，一つの個体の遺伝子，エージェント，適応度，寿命を保持するためのクラス `Creature` を定義しておきます．\n",
    "一つの個体は，送信者エージェントと受信者エージェントのペアで構成されるものとして，初期化時にこれらのエージェントを生成します．このため，初期化時に，これらのエージェントの遺伝子 `bias_genes` を引数として受け取ることとしています．`bias_genes` は，`{'sender': Gene, 'receiver': Gene}` という形式の辞書で，それぞれのエージェントの遺伝子を指定することと想定します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Creature:\n",
    "    def __init__(self, bias_genes, lifespan_mean):\n",
    "        self.bias_genes = bias_genes\n",
    "        self.agents = dict()\n",
    "        for agent_name, bias_gene in bias_genes.items():\n",
    "            self.agents[agent_name] = UrnAgent(bias_gene)\n",
    "        self.fitness = 0\n",
    "        # (1 / lifespan_mean)をパラメータとした幾何分布から寿命をサンプルする\n",
    "        self.lifespan = rng.geometric(1 / lifespan_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZIdvgZD1qae"
   },
   "source": [
    "以下のセルで，具体的に環境と初期集団を生成しています．\n",
    "環境のパラメータを変えて実験してみましょう．\n",
    "\n",
    "また，ここでは，個体同士のペアの組まれやすさの偏り（近接性）も定義しています．\n",
    "創発言語の研究では，集団全体でランダムにペアを組むよりも，一部の個体同士が頻繁にペアを組むような近接性がある方が，より効率的にコミュニケーションが創発することが知られています．\n",
    "また，集団遺伝学におけるマルチレベル選択の理論においても，集団の利益を向上させるような進化において，近接性が重要な役割を果たすことが知られています．\n",
    "この近接性を定義するために，以下のコードでは，それぞれの送信者が，どの受信者とペアになりやすいかを表す確率を定義しています．\n",
    "これについても，色々なパターンを変えて実験してみましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 4          # 状態数\n",
    "num_messages = 4        # 信号数\n",
    "state_dist = [1.0 / num_states] * num_states # 状態の確率分布\n",
    "env = SignalingGameEnv(num_states, num_messages, state_dist)\n",
    "\n",
    "pop_size = 20           # 集団内の個体数\n",
    "num_episodes_per_creature = 100 # 1世代における1個体あたりのエピソード数\n",
    "min_bias = 0.0          # 遺伝的な行動バイアスの最小値\n",
    "max_bias = 30.0         # 遺伝的な行動バイアスの最大値\n",
    "init_bias = 0.0         # 遺伝的な行動バイアスの初期値．乱数で初期化する場合は'random'\n",
    "success_fitness = {'sender': 0.1, 'receiver': 1} # コミュニケーション成功時の適応度増加量\n",
    "success_reward = {'sender': 1, 'receiver': 1} # コミュニケーション成功時の報酬量\n",
    "tornament_size = 10     # トーナメントサイズ\n",
    "do_crossover = True     # 交叉を行うかどうか．行う場合は親二人，行わない場合は親一人のみを選択\n",
    "mutation_rate = 0.001   # 突然変異率\n",
    "lifespan_mean = 1.0     # 平均寿命（幾何分布のパラメータの逆数）\n",
    "\n",
    "# 近接性の定義\n",
    "matching_bias = 'group'\n",
    "num_groups = 5\n",
    "matching_dists = []\n",
    "if matching_bias == 'uniform':\n",
    "    for s in range(pop_size):\n",
    "        matching_dist = np.ones(pop_size) / pop_size\n",
    "        matching_dists.append(matching_dist)\n",
    "elif matching_bias == 'group':\n",
    "    for s in range(pop_size):\n",
    "        group = s % num_groups\n",
    "        matching_dist = np.zeros(pop_size)\n",
    "        matching_dist[group::num_groups] = 1\n",
    "        matching_dist /= matching_dist.sum()\n",
    "        matching_dists.append(matching_dist)\n",
    "\n",
    "# 初期個体群の生成\n",
    "population = []\n",
    "for _ in range(pop_size):\n",
    "    sender_bias_gene = Gene((num_states, num_messages), min_bias, max_bias, init_bias)\n",
    "    receiver_bias_gene = Gene((num_messages, num_states), min_bias, max_bias, init_bias)\n",
    "    bias_genes = {'sender': sender_bias_gene, 'receiver': receiver_bias_gene}\n",
    "    population.append(Creature(bias_genes, lifespan_mean))\n",
    "\n",
    "# あとで進化過程を確認するためのログ\n",
    "log_success_rate = []\n",
    "log_fitness_mean = []\n",
    "log_fitness_var = []\n",
    "log_rewards_mean = {'sender': [], 'receiver': []}\n",
    "log_bias_suc_fail_diff_mean = {'sender': [], 'receiver': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94lyg8G45Ox_"
   },
   "source": [
    "以下のコードで，進化的強化学習のメインループを実装しています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXL4unAWhG1Y",
    "outputId": "fcd4f367-0cce-4f79-ac75-3da7eb4dff47"
   },
   "outputs": [],
   "source": [
    "num_gen = 1000           # 世代数\n",
    "\n",
    "# 進化計算\n",
    "for gen in range(num_gen):\n",
    "    log_success = []\n",
    "    log_fitness = []\n",
    "    log_rewards = {'sender': [], 'receiver': []}\n",
    "    log_success_choice_biases = {'sender': [], 'receiver': []}\n",
    "    log_fail_choice_biases = {'sender': [], 'receiver': []}\n",
    "    # Running episodes\n",
    "    num_episodes_in_this_gen = num_episodes_per_creature * pop_size\n",
    "    for episode in range(num_episodes_in_this_gen):\n",
    "        # 送信者と受信者の選択\n",
    "        sender_idx = rng.integers(0, pop_size)\n",
    "        sender = population[sender_idx]\n",
    "        receiver = rng.choice(population, p=matching_dists[sender_idx])\n",
    "        creatures = {'sender': sender, 'receiver': receiver}\n",
    "        env.reset()\n",
    "        while not env.done:\n",
    "            agent_name = env.agent_selection\n",
    "            obs = env.observe(agent_name)\n",
    "            agent = creatures[agent_name].agents[agent_name]\n",
    "            action = agent.get_action(obs)\n",
    "            env.step(action)\n",
    "            agent.store_buffer(obs, action)\n",
    "        # 1エピソード終了．成功・不成功に基づいてfitnessとrewardを与え，方策を更新する．\n",
    "        log_success.append(1 if env.success else 0)\n",
    "        for _agent_name in env.agent_names:\n",
    "            # 適応度\n",
    "            fitness = success_fitness[_agent_name] if env.success else 0\n",
    "            creatures[_agent_name].fitness += fitness\n",
    "            # 報酬\n",
    "            reward = success_reward[_agent_name] if env.success else 0\n",
    "            creatures[_agent_name].agents[_agent_name].update_reward(reward)\n",
    "            log_rewards[_agent_name].append(reward)\n",
    "            # 遺伝子のロギング（成功時に，その行動が遺伝的にどれだけバイアスされていたか）\n",
    "            if _agent_name == 'sender':\n",
    "                obs, choice = env.state, env.message\n",
    "            elif _agent_name == 'receiver':\n",
    "                obs, choice = env.message, env.action\n",
    "            choice_bias = creatures[_agent_name].bias_genes[_agent_name].values[obs, choice]\n",
    "            if env.success:\n",
    "                log_success_choice_biases[_agent_name].append(choice_bias)\n",
    "            else:\n",
    "                log_fail_choice_biases[_agent_name].append(choice_bias)\n",
    "            # 方策を更新する\n",
    "            creatures[_agent_name].agents[_agent_name].train()\n",
    "                \n",
    "\n",
    "    # 1世代終了．最も適応度の高い個体の壺のボールの数と遺伝子を表示\n",
    "    best_creature = max(population, key=lambda x: x.fitness)\n",
    "    print(f'gen {gen} best creature (fitness {best_creature.fitness}):')\n",
    "    print('sender:')\n",
    "    print(str_agent(best_creature.agents['sender']))\n",
    "    print('receiver:')\n",
    "    print(str_agent(best_creature.agents['receiver']))\n",
    "    # 適応度のロギング\n",
    "    log_fitness = [creature.fitness for creature in population]\n",
    "    # 次世代の個体群を生成する．\n",
    "    new_population = []\n",
    "    for i in range(pop_size):\n",
    "        # 寿命が来ていない個体はそのまま次世代に引き継ぐ\n",
    "        if population[i].lifespan > 1:\n",
    "            population[i].lifespan -= 1\n",
    "            new_population.append(population[i])\n",
    "            continue\n",
    "        # トーナメント選択で親を選び，突然変異を行う．\n",
    "        tournament = rng.choice(population, tornament_size, replace=False)\n",
    "        if do_crossover:\n",
    "            winners = sorted(tournament, key=lambda x: x.fitness, reverse=True)[:2]\n",
    "            new_genes = dict()\n",
    "            for role in env.agent_names:\n",
    "                new_genes[role] = winners[0].bias_genes[role].crossover(winners[1].bias_genes[role])\n",
    "        else:\n",
    "            winner = max(tournament, key=lambda x: x.fitness)\n",
    "            new_genes = winner.bias_genes\n",
    "        for role in env.agent_names:\n",
    "            new_genes[role].mutate(mutation_rate)\n",
    "        new_population.append(Creature(new_genes, lifespan_mean))\n",
    "    population = new_population\n",
    "\n",
    "    # 進化過程のロギング\n",
    "    log_success_rate.append(np.mean(log_success))\n",
    "    for _agent_name in env.agent_names:\n",
    "        log_fitness_mean.append(np.mean(log_fitness))\n",
    "        log_fitness_var.append(np.var(log_fitness))\n",
    "        log_rewards_mean[_agent_name].append(np.mean(log_rewards[_agent_name]))\n",
    "        suc_bias_mean = np.mean(log_success_choice_biases[_agent_name])\n",
    "        fail_bias_mean = np.mean(log_fail_choice_biases[_agent_name])\n",
    "        log_bias_suc_fail_diff_mean[_agent_name].append(suc_bias_mean - fail_bias_mean)\n",
    "\n",
    "    print(f'gen {gen}: success_rate {log_success_rate[-1]} ' \\\n",
    "          f'fitness_mean {log_fitness_mean[-1]} ' \\\n",
    "          f'fitness_var {log_fitness_var[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muPfYM_XEPy8"
   },
   "source": [
    "以下のコードでは，学習過程における報酬のログをTensorboardで確認できる形式で保存しています．\n",
    "記録している情報は以下の通りです．\n",
    "- `success/success_rate`: 各世代での成功率のエピソードごとの平均と分散\n",
    "- `fitness/fitness_{mean,var}`: 各世代での適応度の個体ごとの平均と分散\n",
    "- `reward/{sender,receiver}_reward_mean`: 各世代での送信者と受信者の報酬のエピソードごとの平均\n",
    "- `bias/success_fail_diff_mean_{sender,receiver}`: 各世代で，成功したエピソードにおける行動の遺伝的バイアス（その行動の遺伝子の値）と失敗したエピソードにおける行動の遺伝的バイアス（その行動の遺伝子の値）との差のエピソードごとの平均．この値が大きいほど，遺伝的なバイアスが成功に寄与していることを示します．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jgfn06-FhNeq"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "settings = f'states{num_states}_messages{num_messages}_' \\\n",
    "            f'popsize{pop_size}_episodes{num_episodes_per_creature}_' \\\n",
    "            f'minbias{min_bias}_maxbias{max_bias}_initbias{init_bias}_' \\\n",
    "            f'success_fitness{success_fitness[\"sender\"]}_{success_fitness[\"receiver\"]}_' \\\n",
    "            f'success_reward{success_reward[\"sender\"]}_{success_reward[\"receiver\"]}_' \\\n",
    "            f'tornament{tornament_size}_crossover{do_crossover}_mutation{mutation_rate}_' \\\n",
    "            f'matching{matching_bias}_groups{num_groups}'\n",
    "writer = SummaryWriter(log_dir=f'./tb/{settings}')\n",
    "for i, (suc, f_m, f_v, r_ms, r_mr, g_bds, gbdr) in enumerate(zip(log_success_rate,\n",
    "                                                    log_fitness_mean,\n",
    "                                                    log_fitness_var,\n",
    "                                                    log_rewards_mean['sender'],\n",
    "                                                    log_rewards_mean['receiver'],\n",
    "                                                    log_bias_suc_fail_diff_mean['sender'],\n",
    "                                                    log_bias_suc_fail_diff_mean['receiver'])):\n",
    "    writer.add_scalar(\"success/success_rate\", suc, i)\n",
    "    writer.add_scalar(\"fitness/fitness_mean\", f_m, i)\n",
    "    writer.add_scalar(\"fitness/fitness_var\", f_v, i)\n",
    "    writer.add_scalar(\"reward/sender_reward_mean\", r_ms, i)\n",
    "    writer.add_scalar(\"reward/receiver_reward_mean\", r_mr, i)\n",
    "    writer.add_scalar(\"bias/success_fail_diff_mean_sender\", g_bds, i)\n",
    "    writer.add_scalar(\"bias/success_fail_diff_mean_receiver\", gbdr, i)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsTPk_ypJNuX"
   },
   "source": [
    "以下のコマンドでTensorboardが起動する．\n",
    "このセルは一度だけ実行し，再実行しないことをお勧めする．\n",
    "新しく追加したデータを再読み込みしたい場合，Tensorboardのコンソール上のリロードボタン（右上の方にあるボタン）を押してデータの再読み込みを行うようにするとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EznA0Z37F_AJ"
   },
   "source": [
    "# 研究課題\n",
    "\n",
    "1. 1世代における1個体あたりのエピソード数（`num_episodes_per_creature`）は，生涯での学習機会の量に対応する．このパラメータを変えて実験を行い，遺伝的進化の速度に与える影響を調べてみよう．学習機会が少ないほど，本能的にコミュニケーションを行うことができない遺伝子への淘汰圧は強まると考えられる．一方で，送信者と受信者の一方の本能だけではコミュニケーションは成功しないため，学習機会が全くない状況では進化が起きにくい可能性もある．Zollman & Smead (2010) https://link.springer.com/article/10.1007/s11098-009-9447-x を参照してみよう．彼らによる指摘と，実験結果との関連を考察してみよう．\n",
    "2. 状態数（`num_states`）と信号数（`num_messages`）を変えて実験を行い，遺伝的進化の速度がどのように変わるか調べてみよう．状態数が多いほどコミュニケーションは成功しにくくなるため，遺伝的なバイアスが重要になり，遺伝的進化が重要になるかもしれない．一方で，状態数が多すぎると，何度やっても最適な遺伝子が見つからなくなると考えられるが，どのくらいの状態数までなら進化によって最適遺伝子が見つかるか（本能的な行動に基づくシグナリングシステムが進化できる限界の状態数はどのくらいか）について考察してみよう．\n",
    "3. 集団内の個体数（`pop_size`）や，近接性の条件（`matching_bias` や `num_groups`）を変えて実験を行い，遺伝的進化の速度に与える影響を調べてみよう．個体数が多いほど，多様な遺伝子を集団内に残すことができるため，局所最適解に陥りにくくなるかもしれない．また，近接性の条件は，集団遺伝学におけるマルチレベル選択の理論（例えば，利他的行動の進化の説明）において重要な条件であるが，このこととの関連を考察してみよう．\n",
    "4. 平均寿命（`lifespan_mean`）を変えて実験を行い，遺伝的進化の速度に与える影響を調べてみよう．長寿なほど多くの学習を行うことができるため，コミュニケーションの成功率は向上するはずである．しかし，長寿なほど，遺伝的なバイアスの影響が相対的に小さくなるため，遺伝的な進化を促す淘汰圧は弱まるかもしれない．\n",
    "5. コミュニケーション成功時の報酬量（`success_reward`）や適応度増加量（`success_fitness`）を変えて実験を行い，遺伝的進化の速度に与える影響を調べてみよう．特に，送信者と受信者で，コミュニケーション成功時の報酬量や適応度増加量に違いがある場合，進化の過程がどのように変化するかを調べてみよう．\n",
    "6. 遺伝的アルゴリズムのパラメータ（`tornament_size` や `mutation_rate`）を変えて実験を行い，遺伝的進化の速度に与える影響を調べてみよう．局所最適解に陥りにくくなるパラメータは，状態数や信号数，平均寿命などとも関連があるかもしれない．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLDrAmqmibfm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
