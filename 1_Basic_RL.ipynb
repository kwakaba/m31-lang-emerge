{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 深層強化学習\n",
        "\n",
        "このノートブックでは，シングルエージェントの深層強化学習の実験を行います．"
      ],
      "metadata": {
        "id": "5FkkIt1QnV89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まず，必要なパッケージをインストールします．"
      ],
      "metadata": {
        "id": "Jx1ECcBYngxX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM73IJF9Zt-u"
      },
      "outputs": [],
      "source": [
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "!pip install \"stable-baselines3[extra]==2.0.0a4\"\n",
        "!pip install swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用するパッケージをインポートします．\n",
        "`gymnasium` は強化学習の環境を提供するライブラリです．\n",
        "`stable_baselines3` は強化学習アルゴリズムを提供するライブラリです．"
      ],
      "metadata": {
        "id": "UAj4Y8eDnk47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo.policies import MlpPolicy\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ],
      "metadata": {
        "id": "2RH6KBTqaPqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "強化学習アルゴリズムを実行する前に，環境を確認しましょう．以下は，`gymnasium` が描画した環境を動画に記録し，それをノートブック上で再生するためのコードです．"
      ],
      "metadata": {
        "id": "aTKjjpDin5eH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Video recording/playing functions based on © 2019 Antonin RAFFIN\n",
        "# https://github.com/araffin/rl-tutorial-jnrr19/blob/sb3/1_getting_started.ipynb\n",
        "import os\n",
        "import base64\n",
        "from pathlib import Path\n",
        "from IPython import display as ipythondisplay\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "def show_videos(video_path=\"\", prefix=\"\"):\n",
        "    html = []\n",
        "    for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append(\n",
        "            \"\"\"<video alt=\"{}\" autoplay\n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>\"\"\".format(\n",
        "                mp4, video_b64.decode(\"ascii\")\n",
        "            )\n",
        "        )\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix=\"\", video_folder=\"videos/\"):\n",
        "    eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
        "    # Start the video at step=0 and record 500 steps\n",
        "    eval_env = VecVideoRecorder(\n",
        "        eval_env,\n",
        "        video_folder=video_folder,\n",
        "        record_video_trigger=lambda step: step == 0,\n",
        "        video_length=video_length,\n",
        "        name_prefix=prefix,\n",
        "    )\n",
        "\n",
        "    obs = eval_env.reset()\n",
        "    for _ in range(video_length):\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "    # Close the video recorder\n",
        "    eval_env.close()"
      ],
      "metadata": {
        "id": "T0ohimlcai4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 環境と強化学習モデルの定義\n",
        "\n",
        "以下のセルで，環境と強化学習モデルを定義します．\n",
        "`gymnasium` であらかじめ用意されている環境は，登録名を指定するだけで環境を作成することができます．以下では，`CartPole-v1` という環境を作成しています．他の環境も試してみましょう．登録されている環境については https://gymnasium.farama.org/environments/classic_control/ などを参照してください．\n",
        "\n",
        "また，それと同時に強化学習モデルも定義しています．\n",
        "ここでは，`PPO` という方策勾配法に基づく強化学習モデルを生成しています．\n",
        "- 第一引数では，方策を表現するニューラルネットワークを指定しています．`MlpPolicy` は，あらかじめ `stable-baselines3` で定義されている方策で，多層パーセプトロンに基づく基本的なニューラルネットワークを使って方策を表現します．\n",
        "- 環境が異なれば，観測や行動の空間が異なるため，それに合わせてニューラルネットワークの形を定義する必要があります．このため，`PPO` のコンストラクタの第二引数で環境 `env` を渡す必要があります．\n",
        "- `learning_rate` や `batch_size`, `n_steps`, `n_epochs` はハイパーパラメータです．これらを変化させることで学習の効率が変化する場合があります．これ以外のハイパーパラメータについては https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html を参照してください．"
      ],
      "metadata": {
        "id": "gutQ-DjypujM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"CartPole-v1\"\n",
        "# env_name = \"LunarLander-v2\"\n",
        "# env_name = \"FrozenLake-v1\"\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "model = PPO(MlpPolicy, env, verbose=1,\n",
        "            learning_rate=0.0003, # 学習率\n",
        "            batch_size=64, # バッチサイズ\n",
        "            n_steps=2048, # 訓練データバッファのサイズ\n",
        "            n_epochs=10, # エポック数\n",
        "            device=\"auto\",\n",
        "            tensorboard_log=\"./tb\"\n",
        "            )"
      ],
      "metadata": {
        "id": "C42JJnWSZ-W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のセルでは，環境の中でエージェントを行動させてみて，描画された環境を動画にして表示します．ここでは，エージェントの方策には，初期状態の（ほぼランダムに動く）強化学習モデルを使っていることになります．また，エピソードごとの平均報酬を計算して表示しています．\n",
        "\n",
        "動画が不連続に変化しているように見える箇所があると思いますが，これはエピソードが終了して初期状態に戻っているためです．`CartPole-v1` の環境の場合，以下のいずれかの条件を満たすとエピソードが終了して，初期状態に戻ります．（ https://gymnasium.farama.org/environments/classic_control/cart_pole/#episode-end ）\n",
        "\n",
        "1. Termination: Pole Angle is greater than ±12°\n",
        "2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
        "3. Truncation: Episode length is greater than 500 (200 for v0)"
      ],
      "metadata": {
        "id": "j0655CtYtFEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show video and calculate mean reward before learning\n",
        "\n",
        "record_video(env_name, model, video_length=500, prefix=f\"ppo-{env_name}\")\n",
        "show_videos(\"videos\", prefix=f\"ppo-{env_name}\")\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100, warn=False)\n",
        "print(f\"mean_reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "id": "Sh6Iq0zBbMix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習\n",
        "\n",
        "以下のコードで，強化学習モデルの学習を行います．具体的には，以下の処理が，指定されたステップ数に達するまで実行されます．\n",
        "- `collect_rollouts`: ハイパーパラメータ `n_steps` で指定したステップ数だけ，現在の方策を使って環境の中で行動します．この時に得られた観測や行動，報酬などのデータを，訓練データバッファに格納します．\n",
        "- `train`: 訓練データバッファに格納されたデータを用いて，方策勾配法に基づいたニューラルネットワークの訓練を行います．"
      ],
      "metadata": {
        "id": "fuSiCmxtuomu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning\n",
        "model.learn(total_timesteps=20000, tb_log_name=\"PPO\")"
      ],
      "metadata": {
        "id": "kAPxp1pzbzJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のセルで，再び動画と平均報酬を表示しています．学習がうまくいって，適切な方策が得られた場合，より上手にタスクを遂行できていることが確認できると思います．"
      ],
      "metadata": {
        "id": "tGdhMFt6xbou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show video and calculate mean reward after learning\n",
        "\n",
        "record_video(env_name, model, video_length=500, prefix=f\"ppo-{env_name}-trained\")\n",
        "show_videos(\"videos\", prefix=f\"ppo-{env_name}-trained\")\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "id": "ketWHb3ccOCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のセルで，学習の過程のログを表示するTensorboardを開きます．\n",
        "例えば，「ep_rew_mean」というログは，エピソードごとの平均報酬の推移を表しています．"
      ],
      "metadata": {
        "id": "BUeoGmR0ZASk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=./tb"
      ],
      "metadata": {
        "id": "tpf6L_cQ-Pg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 演習課題\n",
        "\n",
        "- CartPole以外の環境を試してみなさい．\n",
        "- PPOのハイパーパラメータを変更して，学習に与える影響を確認してみなさい．"
      ],
      "metadata": {
        "id": "NZbgVIPJyYDN"
      }
    }
  ]
}