{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 画像シグナリングゲーム（深層強化学習）\n",
        "\n",
        "このノートブックでは，観測した画像の内容を受信者に伝達する画像シグナリングゲームを対象にして，言語創発の実験を行います．方策勾配法を用いる2体の深層強化学習エージェント（送信者と受信者）を同時に学習させ，確立される信号の性質を観察します．\n",
        "\n",
        "まず，必要なパッケージをインストールして，インポートします．"
      ],
      "metadata": {
        "id": "De2HkHqkre4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"stable-baselines3[extra]==2.3.2\""
      ],
      "metadata": {
        "id": "XQxSSC6TSa6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzIwhMmVg5u2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium\n",
        "from gymnasium import spaces\n",
        "import torch\n",
        "import torchvision\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo.policies import MlpPolicy\n",
        "rng = np.random.default_rng()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "まず，画像シグナリングゲームを表現する強化学習環境（`ImageSignalingGameEnv`）を定義します．\n",
        "送信可能なメッセージの長さ（`c_len`），それぞれのメッセージが取り得る語彙の種類数（`c_voc`），入力に用いるCIFAR-100データセットのクラス名（`classes`）を指定して環境を作成できます．\n",
        "環境の初期化を行う `__init__` メソッドの他に，以下の5つのメソッドを定義しています．\n",
        "\n",
        "- `reset`: 最初からゲームプレイを行えるように環境をリセットします．この時，送信者に与えられる伝達対象の情報がランダムに決定されます．\n",
        "- `observe`: エージェントに与えられる観測を返します．\n",
        "- `step`: エージェントが選択した行動を受け取って，ゲームを進めます．\n",
        "- `observation_space`: エージェントに与えられる観測の形式を返します．\n",
        "- `action_space`: エージェントが選択する行動の形式を返します．"
      ],
      "metadata": {
        "id": "5Evauj16rdnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageSignalingGameEnv():\n",
        "    def __init__(self, c_len, c_voc, classes=['lion', 'rabbit']):\n",
        "        self.c_len = c_len\n",
        "        self.c_voc = c_voc\n",
        "        self.classes = classes\n",
        "        cifar = torchvision.datasets.CIFAR100('dataset', train=False, download=True)\n",
        "        self.class_to_imgs = dict((cls, [img for img, label in cifar if cifar.class_to_idx[cls] == label]) for cls in self.classes)\n",
        "        all_imgs_in_np_array = np.array([np.array(img) for cls, imgs in self.class_to_imgs.items() for img in imgs])\n",
        "        self.pixel_mean = all_imgs_in_np_array.mean(axis=(0, 1, 2))\n",
        "        self.pixel_std = all_imgs_in_np_array.std(axis=(0, 1, 2))\n",
        "        self.agents = ['sender', 'receiver']\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.input_class_idx = rng.choice(len(self.classes))\n",
        "        self.input_class = self.classes[self.input_class_idx]\n",
        "        self.input_img_idx = rng.choice(len(self.class_to_imgs[self.input_class]))\n",
        "        self.input_img = self.class_to_imgs[self.input_class][self.input_img_idx]\n",
        "        self.done = False\n",
        "        self.agent_selection = 'sender'\n",
        "\n",
        "    def observe(self, agent):\n",
        "        if agent == 'sender':\n",
        "            feature = (np.array(self.input_img) - self.pixel_mean) / self.pixel_std # Standardize img\n",
        "            return feature\n",
        "        if agent == 'receiver':\n",
        "            return self.message\n",
        "\n",
        "    def step(self, action):\n",
        "        assert self.done == False, 'WARNING: The game is done. Call reset().'\n",
        "        self.rewards = {'sender': 0.0, 'receiver': 0.0}\n",
        "        agent = self.agent_selection\n",
        "        if agent == 'sender':\n",
        "            self.message = action\n",
        "            self.agent_selection = 'receiver'\n",
        "        elif agent == 'receiver':\n",
        "            self.action = action\n",
        "            if action == self.input_class_idx:\n",
        "                self.rewards['sender'] = 1.0\n",
        "                self.rewards['receiver'] = 1.0\n",
        "            self.done = True\n",
        "\n",
        "    def observation_space(self, agent):\n",
        "        if agent == 'sender':\n",
        "            return spaces.Box(low=-np.inf, high=np.inf, shape=(32, 32, 3), dtype=np.float32)\n",
        "        if agent == 'receiver':\n",
        "            return spaces.MultiDiscrete([self.c_voc] * self.c_len)\n",
        "\n",
        "    def action_space(self, agent):\n",
        "        if agent == 'sender':\n",
        "            return spaces.MultiDiscrete([self.c_voc] * self.c_len)\n",
        "        if agent == 'receiver':\n",
        "            return spaces.Discrete(len(self.classes))"
      ],
      "metadata": {
        "id": "YIrsOemzhZiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "強化学習エージェントにはPPOを用います．しかし，stable-baselines3の `PPO` クラスはシングルエージェント学習用に設計されており，そのままではマルチエージェント学習に用いることができません．\n",
        "このため，ここではマルチエージェント学習のための `MAPPO` クラスを定義します．\n",
        "`MAPPO` クラスは，そのエージェントの観測と行動の形式を確定するために，ダミーの環境（`DummyEnv`）を指定して作成できるようにしています．初期化を行う `__init__` メソッドの他に，以下の4つのメソッドを定義しています．\n",
        "\n",
        "- `get_action`: 観測を受け取って，行動を選択して返します．\n",
        "- `store_buffer`: 観測と状態のペアを受け取って，経験として訓練バッファに保存します．\n",
        "- `update_reward`: 次の自分のターンが回ってくる前に，他のエージェントの行動によって自分が報酬を受け取れる場合があります（例えばシグナリングゲームの場合，送信者が行動した時点では報酬はありませんが，次に受信者が適切な行動をした場合には，送信者にも報酬が与えられます）．これを，直近の行動に対する報酬とみなして，訓練バッファの内容を更新します．\n",
        "- `train`: 現在の訓練バッファの内容を使って，方策を更新します．"
      ],
      "metadata": {
        "id": "xu9QTPUryyNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.utils import obs_as_tensor, configure_logger\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class DummyEnv(gymnasium.Env):\n",
        "    observation_space: spaces.Space\n",
        "    action_space: spaces.Space\n",
        "\n",
        "class MAPPO():\n",
        "    def __init__(self, policy, dummy_env, **kwargs):\n",
        "        self.model = PPO(policy, dummy_env, **kwargs)\n",
        "        self.train_buf = self.model.rollout_buffer\n",
        "        self.obs_shape = dummy_env.observation_space.shape\n",
        "        self.act_shape = dummy_env.action_space.shape\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        obs = obs.reshape((-1,) + self.obs_shape)\n",
        "        with torch.no_grad():\n",
        "            obs_tensor = obs_as_tensor(obs, self.model.policy.device)\n",
        "            actions, values, log_probs = self.model.policy.forward(obs_tensor)\n",
        "        return actions.cpu().numpy()[0], values, log_probs\n",
        "\n",
        "    def store_buffer(self, obs, action, dones, values, log_probs):\n",
        "        self.train_buf.add(\n",
        "            np.reshape(obs, (1,) + self.obs_shape),\n",
        "            np.reshape(action, (1,) + self.act_shape),\n",
        "            [0], dones, values, log_probs)\n",
        "        self._last_values = values\n",
        "\n",
        "    def update_reward(self, reward):\n",
        "        self.train_buf.rewards[self.train_buf.pos - 1][0] += reward\n",
        "\n",
        "    def train(self, values=None, dones=None):\n",
        "        if values is None:\n",
        "            values = self._last_values\n",
        "            dones = np.ones(len(values))\n",
        "        self.train_buf.compute_returns_and_advantage(values, dones)\n",
        "        self.model.train()\n",
        "        self.train_buf.reset()"
      ],
      "metadata": {
        "id": "4Eg0SJxZiWVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のセルで，具体的に環境と強化学習モデルを定義します．\n",
        "環境のパラメータを変えて実験してみましょう．"
      ],
      "metadata": {
        "id": "gZIdvgZD1qae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['lion', 'rabbit']   # 入力に用いるクラス名\n",
        "c_len = 5   # メッセージの長さ\n",
        "c_voc = 2   # メッセージの種類数\n",
        "env = ImageSignalingGameEnv(c_len, c_voc, classes)\n",
        "\n",
        "tensorboard_log = './tb'\n",
        "n_steps = 2048            # 訓練データバッファのサイズ\n",
        "\n",
        "agents = {}\n",
        "for agent_name in env.agents:\n",
        "    dummy_env = DummyEnv(env.observation_space(agent_name), env.action_space(agent_name))\n",
        "    agent = MAPPO(MlpPolicy, dummy_env, verbose=1,\n",
        "                learning_rate=0.0003, # 学習率\n",
        "                batch_size=64, # バッチサイズ\n",
        "                n_epochs=10, # エポック数\n",
        "                n_steps=n_steps,\n",
        "                device=\"auto\"\n",
        "                )\n",
        "    tb_log_name = f'{agent_name}_{classes}_c_len{c_len}_c_voc{c_voc}'\n",
        "    logger = configure_logger(True, tensorboard_log, tb_log_name, True)\n",
        "    agent.model._logger = logger\n",
        "    agents[agent_name] = agent\n",
        "\n",
        "total_timesteps = 50000  # ゲームプレイ回数"
      ],
      "metadata": {
        "id": "ZTLmKnJahLxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のコードでは，入力として使われる画像をいくつかサンプルして表示しています．"
      ],
      "metadata": {
        "id": "VjxiJ-jAtKfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = (1, 1)\n",
        "for _ in range(5):\n",
        "    env.reset()\n",
        "    print(env.input_class)\n",
        "    plt.imshow(env.input_img)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4g3rnPXpSsYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のコードで，2体の強化学習モデルの学習を行います．\n",
        "学習過程における報酬のログも，Tensorboardで確認できる形式で保存しています．"
      ],
      "metadata": {
        "id": "94lyg8G45Ox_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for timestep in range(total_timesteps):\n",
        "    # ゲームプレイ開始\n",
        "    env.reset()\n",
        "    while not env.done:\n",
        "        agent_name = env.agent_selection\n",
        "        agent = agents[agent_name]\n",
        "        obs = env.observe(agent_name)\n",
        "        action, values, log_probs = agent.get_action(obs)\n",
        "        env.step(action)\n",
        "        agent.store_buffer(obs, action, [env.done], values, log_probs)\n",
        "        for _agent_name in env.agents:\n",
        "            reward = env.rewards[_agent_name]\n",
        "            agents[_agent_name].update_reward(reward)\n",
        "    # ゲームプレイ終了．方策を更新する\n",
        "    if (timestep + 1) % n_steps == 0:\n",
        "        for _agent_name in env.agents:\n",
        "            _agent = agents[_agent_name]\n",
        "            mean_reward = np.mean([r[0] for r in _agent.train_buf.rewards])\n",
        "            _agent.model.logger.record(\"rollout/ep_rew_mean\", mean_reward)\n",
        "            _agent.model.logger.record(\"time/total_timesteps\", timestep, exclude=\"tensorboard\")\n",
        "            _agent.model.logger.dump(timestep)\n",
        "            _agent.train()"
      ],
      "metadata": {
        "id": "sXL4unAWhG1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のコードで，学習した結果の方策を用いたときの，ゲームプレイごとの平均報酬を評価します．\n",
        "コミュニケーションが成功すれば報酬1，失敗すれば報酬0なので，平均報酬の値は，コミュニケーションの成功率ということになります．\n",
        "\n",
        "また，いくつかの入力画像に対して，送信者が送信したメッセージを表示しています．メッセージと画像の対応関係がどのようになっていると考えられるか，考察してみましょう．"
      ],
      "metadata": {
        "id": "wgoyKPA07rRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate policy\n",
        "n_eval_episodes = 100\n",
        "sender_rewards = []\n",
        "receiver_rewards = []\n",
        "imgs = []\n",
        "messages = []\n",
        "for _ in range(n_eval_episodes):\n",
        "    env.reset()\n",
        "    while not env.done:\n",
        "        agent_name = env.agent_selection\n",
        "        obs = env.observe(agent_name)\n",
        "        action, values, log_probs = agents[agent_name].get_action(obs)\n",
        "        if agent_name == 'sender':\n",
        "            imgs.append(env.input_img)\n",
        "            messages.append(action)\n",
        "        env.step(action)\n",
        "    sender_rewards.append(env.rewards['sender'])\n",
        "    receiver_rewards.append(env.rewards['receiver'])\n",
        "print(f'Sender average reward: {np.mean(sender_rewards):.2f}')\n",
        "print(f'Receiver average reward: {np.mean(receiver_rewards):.2f}')\n",
        "# Sample some of imgs and messages and show them\n",
        "for i in range(10):\n",
        "    print(messages[i])\n",
        "    plt.imshow(imgs[i])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sNgOdHsd4gQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のコマンドでTensorboardが起動します．\n",
        "このセルは一度だけ実行し，再実行しないことをお勧めします．\n",
        "新しく追加したデータを再読み込みしたい場合は，Tensorboardのコンソール上のリロードボタン（右上の方にあるボタン）を押してデータの再読み込みを行うようにするとよいです．"
      ],
      "metadata": {
        "id": "RsTPk_ypJNuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=./tb"
      ],
      "metadata": {
        "id": "1_058joe-JZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 演習課題\n",
        "\n",
        "- 入力画像のクラス（`classes`）に，別のクラスを指定したり，使用するクラスの数を3つ以上指定したときに，コミュニケーションの成功率がどのように変化するか調べてみよう．CIFAR-100データセットに含まれるクラスの一覧は https://www.cs.toronto.edu/~kriz/cifar.html に記載されている．\n",
        "- メッセージの長さ（`c_len`）と語彙数（`c_voc`）を変化させたときに，コミュニケーションの成功率がどのように変化するか調べてみよう．\n"
      ],
      "metadata": {
        "id": "N9AsH-g_zpsP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TFw-RuFk_spc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}